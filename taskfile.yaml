version: '3'

env:
  PYTHON: /usr/bin/python3.11
  DEVICE: cuda:0

tasks:

  dependencies: 
    sources:
      - requirements.txt
    generates:
      - venv/.builded
    cmds:
      - rm -rf venv
      - $PYTHON -m venv venv
      - venv/bin/python3 -m pip install -r requirements.txt
      - touch venv/.builded

  ppo-scattered:
    deps: 
      - dependencies
    sources:
      - src/**/*.py
    generates:
      - models/ppo/.trained
    cmds:
      - mkdir -p models/ppo
      - venv/bin/python3 src/cli.py
        cli storage     default
        cli agent       transformer-agent
        cli environment scattered
        cli algorithm   ppo 
        cli algorithm   ppo trajectory default --steps 32
        cli algorithm   ppo optimizer  adam
        cli algorithm   ppo scheduler  cosine
        cli logger      file --path models/ppo/train.log
        cli callback    base
        cli callback    checkpointer --path models/ppo/agent.pkl 
        cli save-configuration    --path models/ppo/configuration.json
        cli train 
      - cp ./taskfile.yaml models/ppo/
      - touch models/ppo/.trained

  shac-learnable-reward:
    deps: 
      - dependencies
    sources:
      - src/**/*.py
    generates:
      - models/shac-learnable-reward/.trained
    cmds:
      - mkdir -p models/shac-learnable-reward
      - venv/bin/python3 src/cli.py
        cli storage     default
        cli agent       transformer-agent
        cli environment scattered-learnable-reward 
        cli environment scattered-learnable-reward reward transformer-reward
        cli environment scattered-learnable-reward optimizer adam --learning-rate 0.0001
        cli environment scattered-learnable-reward scheduler constant
        cli algorithm   shac 
        cli algorithm   shac trajectory default 
        cli algorithm   shac trajectory optimizer adam --learning-rate 0.0005
        cli algorithm   shac trajectory scheduler constant 
        cli algorithm   shac optimizer  adam --learning-rate 0.0001
        cli algorithm   shac scheduler  cosine --lrmin 0.000001
        cli logger      file --path models/shac-learnable-reward/train.log
        cli callback    base-shac-learnable-reward
        cli callback    checkpointer --path models/shac-learnable-reward/agent.pkl 
        cli save-configuration --path models/shac-learnable-reward/configuration.json
        cli train 
      - cp ./taskfile.yaml models/shac-learnable-reward/
      - touch models/shac-learnable-reward/.trained

  ppo-dispersion:
    deps: 
      - dependencies
    sources:
      - src/**/*.py
    generates:
      - models/ppo-dispersion/.trained
    cmds:
      - mkdir -p models/ppo-dispersion
      - venv/bin/python3 src/cli.py
        cli storage     default
        cli agent       mlp-agent --observation-size 13 --action-size 2 
        cli environment dispersion --envs 512 --grad-enabled False --agents 3
        cli algorithm   ppo --batch-size 2048 --epochs 4
        cli algorithm   ppo trajectory default --steps 64
        cli algorithm   ppo optimizer  adam --learning-rate 0.001
        cli algorithm   ppo scheduler  constant
        cli logger      file --path models/ppo-dispersion/train.log
        cli callback    dispersion
        cli callback    checkpointer --path models/ppo-dispersion/agent.pkl --utc 10
        cli save-configuration --path models/ppo-dispersion/configuration.json
        cli train --epochs 1000000
      - cp ./taskfile.yaml models/ppo-dispersion/
      - touch models/ppo-dispersion/.trained



  ppo-learnable-reward:
    deps: 
      - dependencies
    sources:
      - src/**/*.py
    generates:
      - models/ppo-lernable-reward/.trained
    cmds:
      - mkdir -p models/ppo-learnable-reward
      - venv/bin/python3 src/cli.py
        cli agent transformer-agent
        cli environment scattered-learnable-reward
        cli environment reward transformer-reward 
        cli loss ppo
        cli optimizer adam
        cli scheduler cosine
        cli trajectory ppo
        cli storage default
        cli logger file --path models/ppo-learnable-reward/train.log
        cli callback base-learnable-reward
        cli callback checkpointer --path models/ppo-learnable-reward/agent.pkl 
        cli save-configuration --path models/ppo-learnable-reward/configuration.json
        cli train
      - cp ./taskfile.yaml models/ppo-learnable-reward/
      - touch models/ppo-learnable-reward/.trained

  viz:
    deps: [dependencies]
    cmds:
      - venv/bin/python3 src/cli.py
        cli agent transformer-agent --state-dict-path models/ppo/agent.pkl
        cli environment scattered --envs 1
        cli viz --show True

  fig:
    deps:
      - dependencies
    cmds:
      - jet init --shape 1 1
        jet line --samples 1000 --input-path models/ppo/train.log                   --x id --y message/reward      --label ppo
        jet line --samples 1000 --input-path models/ppo-learnable-reward/train.log  --x id --y message/real_reward --label ppo-learnable-reward
        jet line --samples 1000 --input-path models/shac-learnable-reward/train.log --x id --y message/real_reward --label shac-learnable-reward
        jet plot --show True

  clean-ppo:
    cmds:
      - rm -rf models/ppo


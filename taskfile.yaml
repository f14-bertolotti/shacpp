version: '3'

env:
  PYTHON: /usr/bin/python3.11
  DEVICE: cuda:0

includes:
  dispersion: 
    taskfile: tasks/dispersion/taskfile.yaml
    dir: .
    vars: 
      EPISODES: 5000 

tasks:
  dependencies: 
    sources:
      - requirements.txt
    generates:
      - venv/.builded
    cmds:
      - rm -rf venv
      - $PYTHON -m venv venv
      - venv/bin/python3 -m pip install -r requirements.txt
      - touch venv/.builded

#  ppo-scattered:
#    deps: 
#      - dependencies
#    sources:
#      - src/**/*.py
#    generates:
#      - models/ppo/.trained
#    cmds:
#      - mkdir -p models/ppo
#      - venv/bin/python3 src/cli.py
#        cli storage     default
#        cli agent       transformer-agent
#        cli environment scattered
#        cli algorithm   ppo 
#        cli algorithm   ppo trajectory default --steps 32
#        cli algorithm   ppo optimizer  adam
#        cli algorithm   ppo scheduler  cosine
#        cli logger      file --path models/ppo/train.log
#        cli callback    base
#        cli callback    checkpointer --path models/ppo/agent.pkl 
#        cli save-configuration    --path models/ppo/configuration.json
#        cli train 
#      - cp ./taskfile.yaml models/ppo/
#      - touch models/ppo/.trained
#
#  shac-learnable-reward:
#    deps: 
#      - dependencies
#    sources:
#      - src/**/*.py
#    generates:
#      - models/shac-learnable-reward/.trained
#    cmds:
#      - mkdir -p models/shac-learnable-reward
#      - venv/bin/python3 src/cli.py
#        cli storage     default
#        cli agent       transformer-agent
#        cli environment scattered-learnable-reward 
#        cli environment scattered-learnable-reward reward transformer-reward
#        cli environment scattered-learnable-reward optimizer adam --learning-rate 0.0001
#        cli environment scattered-learnable-reward scheduler constant
#        cli algorithm   shac 
#        cli algorithm   shac trajectory default 
#        cli algorithm   shac trajectory optimizer adam --learning-rate 0.001
#        cli algorithm   shac trajectory scheduler constant 
#        cli algorithm   shac optimizer  adam --learning-rate 0.0001
#        cli algorithm   shac scheduler  cosine --lrmin 0.000001
#        cli logger      file --path models/shac-learnable-reward/train.log
#        cli callback    base-shac-learnable-reward
#        cli callback    checkpointer --path models/shac-learnable-reward/agent.pkl 
#        cli save-configuration --path models/shac-learnable-reward/configuration.json
#        cli train 
#      - cp ./taskfile.yaml models/shac-learnable-reward/
#      - touch models/shac-learnable-reward/.trained
#
#  ppo-dispersion:
#    deps: 
#      - dependencies
#    sources:
#      - src/**/*.py
#    vars:
#      FOLDER : '{{default "ppo-dispersion" .FOLDER}}'
#    generates:
#      - models/{{.FOLDER}}/.trained
#    cmds:
#      - mkdir -p models/{{.FOLDER}}
#      - venv/bin/python3 src/cli.py
#        cli environment dispersion --envs 128 --agents 3 --seed 42 --shared-reward False
#        cli agent mlp --compile True  --hidden-size 128 --shared True 
#        cli algorithm ppo --batch-size 2048 --epochs 5
#        cli algorithm ppo trajectory default --steps 48
#        cli algorithm ppo optimizer  adam --learning-rate 0.001
#        cli algorithm ppo scheduler  constant
#        cli callback trainlog           --path models/{{.FOLDER}}/train.log
#        cli callback validate           --path models/{{.FOLDER}}/valid.log --ete 10 --steps 64 --size 512
#        cli callback checkpointer       --path models/{{.FOLDER}}/agent.pkl --ete 10
#        cli callback save-configuration --path models/{{.FOLDER}}/configuration.json
#        cli callback save-best          --path models/{{.FOLDER}}/best.pkl
#        cli callback bar 
#        cli train --episodes 10000 --seed 42 
#      - cp ./taskfile.yaml models/{{.FOLDER}}/
#      - touch models/{{.FOLDER}}/.trained
#
#  shac-dispersion:
#    deps: 
#      - dependencies
#    sources:
#      - src/**/*.py
#    vars:
#      FOLDER : '{{default "shac-dispersion" .FOLDER}}'
#    generates:
#      - models/{{.FOLDER}}/.trained
#    cmds:
#      - mkdir -p models/{{.FOLDER}}
#      - venv/bin/python3 src/cli.py
#        cli environment dispersion --device cuda:0 --envs 128 --agents 3 --seed 42 --requires-grad True
#        cli agent mlp --compile True --hidden-size 128 --shared False --actor-init-gain 1.41
#        cli algorithm shac --batch-size 2048 --epochs 8 --target-alpha 0.2
#        cli algorithm shac critic-optimizer adam --learning-rate 0.001 
#        cli algorithm shac actor-optimizer  adam --learning-rate 0.001 
#        cli algorithm shac critic-scheduler constant  
#        cli algorithm shac actor-scheduler  constant
#        cli algorithm shac trajectory default --steps 32
#        cli callback trainlog           --path models/{{.FOLDER}}/train.log
#        cli callback validate           --path models/{{.FOLDER}}/valid.log --ete 10 --steps 64 --size 512
#        cli callback checkpointer       --path models/{{.FOLDER}}/agent.pkl --ete 10
#        cli callback save-configuration --path models/{{.FOLDER}}/configuration.json
#        cli callback save-best          --path models/{{.FOLDER}}/best.pkl
#        cli callback bar 
#        cli train --episodes 10000 --seed 42 
#      - cp ./taskfile.yaml models/{{.FOLDER}}/
#      - touch models/{{.FOLDER}}/.trained
#
#
#  shac-dispersion-sr:
#    deps: 
#      - dependencies
#    sources:
#      - src/**/*.py
#    vars:
#      FOLDER : '{{default "shac-dispersion-sr" .FOLDER}}'
#    generates:
#      - models/{{.FOLDER}}/.trained
#    cmds:
#      - mkdir -p models/{{.FOLDER}}
#      - venv/bin/python3 src/cli.py
#        cli environment proxied dispersion --device cuda:0 --envs 128 --agents 3 --seed 42 --requires-grad True
#        cli environment proxied dispersion mlp --hidden-size 256 --layers 3 --dropout 0.1 --activation ReLU
#        cli environment proxied dispersion optimizer adam --learning-rate 0.0001
#        cli environment proxied dispersion scheduler constant
#        cli agent mlp --compile True --hidden-size 128 --shared False --actor-init-gain 1.41
#        cli algorithm shac --batch-size 1024 --epochs 8 --target-alpha 0.4
#        cli algorithm shac critic-optimizer adam --learning-rate 0.001 
#        cli algorithm shac actor-optimizer  adam --learning-rate 0.001 
#        cli algorithm shac critic-scheduler constant  
#        cli algorithm shac actor-scheduler  constant
#        cli algorithm shac trajectory default --steps 32 --utr 5
#        cli callback greeting
#        cli callback barline
#        cli callback trainlog         --path    models/{{.FOLDER}}/train.log
#        cli callback checkpoint-agent --path    models/{{.FOLDER}}/agent.pkl --etc 100
#        cli callback saveconfig       --path    models/{{.FOLDER}}/config.json
#        cli callback proxied proxylog --path    models/{{.FOLDER}}/proxy.log
#        cli callback proxied validate --sdpath  models/{{.FOLDER}}/best.pkl 
#                                      --logpath models/{{.FOLDER}}/valid.log
#                                      --etc 10 --size 512 --steps 64
#        cli train --episodes 10000 --seed 42 
#      - cp ./taskfile.yaml models/{{.FOLDER}}/
#      - touch models/{{.FOLDER}}/.trained
#
#  ppo-transport:
#    deps: 
#      - dependencies
#    sources:
#      - src/**/*.py
#    vars:
#      FOLDER : '{{default "ppo-transport" .FOLDER}}'
#    generates:
#      - models/{{.FOLDER}}/.trained
#    cmds:
#      - mkdir -p models/{{.FOLDER}}
#      - venv/bin/python3 src/cli.py
#        cli environment transport --envs 128 --agents 3 --seed 42
#        cli agent mlp --compile True  --hidden-size 128 --shared False   
#        cli algorithm ppo --batch-size 2048 --epochs 5
#        cli algorithm ppo trajectory default --steps 48 --utr 4
#        cli algorithm ppo optimizer  adam --learning-rate 0.001
#        cli algorithm ppo scheduler  constant
#        cli callback trainlog           --path models/{{.FOLDER}}/train.log
#        cli callback validate           --path models/{{.FOLDER}}/valid.log --ete 10 --steps 64 --size 512
#        cli callback checkpointer       --path models/{{.FOLDER}}/agent.pkl --ete 10
#        cli callback save-configuration --path models/{{.FOLDER}}/configuration.json
#        cli callback save-best          --path models/{{.FOLDER}}/best.pkl
#        cli callback bar 
#        cli train --episodes 10000 --seed 42 
#      - cp ./taskfile.yaml models/{{.FOLDER}}/
#      - touch models/{{.FOLDER}}/.trained
#
#  dispersion-viz:
#    deps:
#      - dependencies
#    sources:
#      - models/**/*.log
#    generates:
#      - models/dispersion.png
#    vars:
#      SAMPLES: '{{default 1000 .SAMPLES}}'
#    cmds:
#      - SAMPLES={{.SAMPLES}}; eval "jet init --shape 1 1 "$(find models -maxdepth 1 -type d -name '*dispersion*' -printf " jet line --samples ${SAMPLES} --input-path %p/valid.log --x message/episode --y message/reward --label %p")" jet plot --show True --output-path models/dispersion.png"
#
#  transport-viz:
#    deps:
#      - dependencies
#    sources:
#      - models/**/*.log
#    generates:
#      - models/transport.png
#    vars:
#      SAMPLES: '{{default 1000 .SAMPLES}}'
#    cmds:
#      - SAMPLES={{.SAMPLES}}; eval "jet init --shape 1 1 "$(find models -maxdepth 1 -type d -name '*transport*' -printf " jet line --samples ${SAMPLES} --input-path %p/valid.log --x message/episode --y message/reward --label %p")" jet plot --show True --output-path models/transport.png"


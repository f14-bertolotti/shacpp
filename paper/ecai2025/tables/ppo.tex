\begin{tabular}{ l l l}
	\toprule
	Parameter & Transformer & MLP \\
	\midrule
	number of training environments & 512 & 512 \\
	training horizon & 32 & 32 \\
	number of evaluation environments & 512 & 512 \\
	evaluation horizon & 512 & 512 \\
	policy layers & 1 & 1 \\
	policy hidden size & 64 & {160, 32, 96} \\
	policy feedforward size & 128 & - \\
	policy heads & 1 & - \\
	policy dropout & 0.1 & 0.1 \\
	policy activation & ReLU & ReLU \\
	policy variance & 1 & 1 \\
	value layers & 1 & 1 \\
	value hidden size & 64 & {160, 32, 96} \\
	value feedforward size & 128 & - \\
	value dropout & 0.0 & 0.0 \\
	value activation & ReLU & ReLU \\
	policy learning rate & 0.001 & 0.001 \\
	value learning rate & 0.001 & 0.001 \\
	early stopping - max reward fraction & 0.9 & 0.9 \\
	early stopping - max envs fraction & 0.9 & 0.9 \\
	seed & {42, 43, 44} & {42, 43, 44} \\
	max episodes & 20000 & 20000 \\
	epochs between environment resets & 10 & 10 \\
	epochs between evaluations & 100 & 100 \\
	$\gamma$ & 0.99 & 0.99 \\
	$\lambda$ & 0.95 & 0.95 \\
	$\alpha$ & 1.0 & 1.0 \\
	\bottomrule
\end{tabular}

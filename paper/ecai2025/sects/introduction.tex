
\section{Introduction}\label{sect:introduction}
Learning control policies is crucial for applications like robotics \cite{Singh22}, autonomous driving \cite{Elallid22}, and swarm intelligence \cite{Tang21}. These domains often present challenges such as sparse rewards, complex dynamics, and multi-agent interactions. Many current RL approaches focus specifically on either single- or multi-agent settings, lacking a unified solution. Furthermore, RL algorithms are often \emph{sample inefficient}, requiring extensive interaction with the environment to learn effective policies.

A promising direction to improve sample efficiency involves utilizing \emph{differentiable simulators}, namely simulators that allow for backpropagation through their components. SHAC \cite{Xu22} exemplifies this approach, using gradients obtained directly from the simulator to accelerate policy optimization. However, SHAC's applicability is constrained by two key limitations. First, it necessitates both a differentiable transition function \emph{and} a differentiable reward function, which is often impractical, especially with sparse or complex reward structures. Second, while powerful, backpropagation through complex dynamics, like object collisions~\cite{Georgiev24} or intricate multi-agent interactions, can lead to unstable gradients~\cite{Bengio94, Metz21}, potentially hindering learning. The original SHAC work also focused primarily on single-agent tasks.

In this work, we propose \fname{} to address these limitations and extend gradient-based policy optimization beyond SHAC's constraints. Our goal is to develop a sample-efficient reinforcement learning framework that maintains the benefits of gradient-based optimization without requiring differentiable simulators. Drawing inspiration from model-based RL \cite{DBLP:conf/icml/HafnerLFVHLD19, DBLP:conf/iclr/HafnerLB020}, \fname{} substitutes direct simulator differentiation with neural approximations of the gradient functions. By training networks that approximate transition and reward gradients concurrently with policy optimization, our approach preserves the sample efficiency benefits of gradient information while eliminating strict differentiability requirements. This design enhances robustness to complex dynamics and enables application in both non-differentiable environments and multi-agent scenarios, albeit with additional computational overhead compared to SHAC.

We evaluate \fname{} against two established baselines: PPO \cite{Schulman17} (including its multi-agent variant MAPPO \cite{DBLP:conf/nips/YuVVGWBW22}) as the standard for non-differentiable environments, and the original SHAC \cite{Xu22} where applicable. Our experiments utilize the VMAS simulator \cite{DBLP:conf/dars/BettiniKBP22}, which provides physics-based multi-agent environments with varying levels of complexity. Through this comparative analysis on cooperative tasks with different agent populations, we investigate the following research questions:

\begin{compactitem}
    \item \RQ{1} Can we train a neural network to approximate the gradients of a differentiable simulator?
    \item \RQ{2} How does our algorithm compare to PPO and SHAC in both single-agent and multi-agent settings?
    \item \RQ{3} How does the performance of these algorithms change as the search space increases?
\end{compactitem}

\noindent Our contributions include:
\begin{compactitem}
    \item a novel RL framework (\fname{}) that employs learned gradient approximations, eliminating the need for differentiable simulators;
    \item the first empirical evaluation of SHAC in multi-agent environments, establishing a baseline for differentiable MARL;
    \item a comprehensive comparison across environments with varying agent counts and differentiability properties;
    \item experimental evidence demonstrating that \fname{} matches SHAC's performance in differentiable settings while substantially outperforming PPO in sample efficiency and final reward across both single-agent and complex multi-agent scenarios.
\end{compactitem}

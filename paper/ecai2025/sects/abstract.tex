\begin{abstract}
Reinforcement learning (RL) algorithms show promise in robotics and multi-agent systems but often suffer from low sample efficiency. While methods like SHAC leverage differentiable simulators to improve efficiency, they are limited to specific settings: they require fully differentiable environments (including transition and reward functions) and have primarily been demonstrated in single-agent scenarios. To overcome these limitations, we introduce \fname{}, a novel framework inspired by SHAC\@. \fname{} removes the need for differentiable simulator components by using neural networks to approximate the required gradients, training these networks alongside the standard policy and value networks. This allows the core SHAC approach to be applied in both non-differentiable and multi-agent environments. We evaluate \fname{} on challenging multi-agent tasks from the VMAS suite, comparing it against SHAC (where applicable) and PPO, a standard algorithm for non-differentiable settings. Our results demonstrate that \fname{} significantly outperforms PPO in both single- and multi-agent scenarios. Furthermore, in differentiable environments where SHAC operates, \fname{} achieves comparable performance despite lacking direct access to simulator gradients, thus successfully extending SHAC's benefits to a broader class of problems
\end{abstract}

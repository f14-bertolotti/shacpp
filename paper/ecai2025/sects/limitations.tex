\section{Limitations}

Environments often exhibit vastly different characteristics and challenges, and the proposed method is not a universal solution suitable for all scenarios. One of the most difficult components to learn is the world model, which relies on meaningful interactions between agents and the environment—interactions that do not necessarily yield immediate rewards. This creates a cold-start problem: the world model cannot be learned until agents are able to interact effectively with the environment. To address this, as in prior works, we use stochastic action sampling instead of a deterministic policy. In the environments considered, this approach eventually leads to the successful learning of a world model, which in turn allows agents to develop a viable policy. However, this strategy may not be effective in more complex environments where random actions fail to produce meaningful progress. In such cases, the proposed method may not succeed in learning a policy.

This limitation can be mitigated by introducing a cold-start dataset—either manually curated or generated using alternative algorithms such as PPO or MPPI~\cite{Alvarez24}. Such data can help initialize the world model and provide a useful foundation for the learning process.

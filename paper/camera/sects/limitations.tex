\section{Limitations}

While the proposed method shows promise, it is not a one-size-fits-all solution. Environments vary significantly in complexity and dynamics, and several limitations must be acknowledged.

A central challenge lies in learning the world model, which depends on rich, meaningful interactions between agents and the environment. These interactions often yield sparse or delayed rewards, leading to a cold-start problem: until the world model is sufficiently trained, agents may struggle to explore or act effectively, even when guided by stochastic policies. This issue can be partially addressed by seeding the learning process with a cold-start dataset—either manually designed or generated using alternative algorithms such as PPO or MPPI~\cite{Alvarez24}. Such data helps bootstrap the world model, providing a more stable foundation for training.

Despite careful attention to implementation, including reference to best practices (e.g., the well-known "37 Implementation Details of Proximal Policy Optimization" blog~\cite{Shengyi22}), we observed that PPO and MAPPO performed suboptimally in our experiments. To ensure the validity of these findings, we cross-verified results using a third-party implementation~\cite{Bettini24}, which yielded consistent outcomes.

Although early stopping is primarily used to conserve computational resources, it can give the false impression that the algorithm is inherently robust to policy collapse. In reality, we observed multiple instances of partial policy collapse—some of which recovered—and a few cases of complete collapse without recovery. One example of partial collapse appears in the five-agent variant of the dispersion scenario shown in \Cref{apx:fig:experiments-mlp}.

The discovery scenario itself poses considerable difficulty for all tested algorithms due to its inherent partial observability. Agents rely solely on a simulated LiDAR system for perception, which provides a narrow, local view of the environment. This makes it particularly difficult for any method—including \fname{}—to construct a complete and accurate world model, often resulting in suboptimal performance. Ultimately, this may indicate that partial observability may be challenging to overcome with the current approach.

\fname{} addresses the limitations of SHAC by introducing additional training components, specifically the transition and reward networks. However, this improvement comes at the cost of increased computational overhead. The training burden is especially pronounced in environments with a large number of agents, where the transition network must capture complex inter-agent dynamics. While this trade-off is acceptable in many scenarios, it significantly slows down the training process and demands computational resources that may not be readily available in all settings.

Regarding interpretability, \fname{} does not offer significant advantages over its predecessors. As the policy is governed by a neural network, understanding or explaining its behavior remains a challenging task. This limitation is inherent to most deep reinforcement learning methods and highlights the ongoing need for techniques that enhance policy transparency and interpretability.



\section{Introduction}\label{sect:introduction}
Learning control policies is crucial for applications such as robotics \cite{Singh22}, autonomous driving \cite{Elallid22}, and swarm intelligence \cite{Tang21}. These domains often present challenges including sparse rewards, complex dynamics, and multi-agent interactions. Many current RL approaches focus specifically on either single- or multi-agent settings, lacking a unified solution. Furthermore, RL algorithms are often \emph{sample inefficient}, requiring extensive interaction with the environment to learn effective policies.

A promising direction to improve sample efficiency involves utilizing \emph{differentiable simulators}, namely simulators that allow backpropagation through their components. SHAC \cite{Xu22} exemplifies this approach by using gradients obtained directly from the simulator to accelerate policy optimization. However, SHACâ€™s applicability is constrained by two key limitations. First, it necessitates both a differentiable transition function \emph{and} a differentiable reward function, which is often impractical, especially in the presence of sparse or complex reward structures. Second, while powerful, backpropagation through complex dynamics, such as object collisions~\cite{Georgiev24} or intricate multi-agent interactions, can lead to unstable gradients~\cite{Bengio94, Metz21}, potentially hindering learning. The original SHAC work also focused primarily on single-agent tasks.

In this work, we propose \fname{} to address these limitations and extend gradient-based policy optimization beyond SHAC's constraints. Our goal is to develop a sample-efficient reinforcement learning framework that maintains the benefits of gradient-based optimization without requiring differentiable simulators. Drawing inspiration from model-based RL \cite{DBLP:conf/icml/HafnerLFVHLD19, DBLP:conf/iclr/HafnerLB020}, \fname{} substitutes direct simulator differentiation with neural approximations by training networks that approximate the transition and reward functions concurrently with the policy function. This approach preserves the benefits of gradient information while eliminating strict differentiability requirements. It also enhances robustness to complex dynamics and enables application in both non-differentiable environments and multi-agent scenarios, albeit with additional computational overhead compared to SHAC.

We evaluate \fname{} against two established baselines: PPO \cite{Schulman17} (including its multi-agent variant MAPPO \cite{DBLP:conf/nips/YuVVGWBW22}), as the standard for non-differentiable environments, and the original SHAC \cite{Xu22}, where applicable. Our experiments utilize the VMAS simulator \cite{DBLP:conf/dars/BettiniKBP22}, which provides physics-based multi-agent environments with varying levels of complexity. Through this comparative analysis on cooperative tasks with different agent populations, we investigate the following research questions:

\begin{compactitem}
    \item \RQ{1} Can we train a neural network to approximate the gradients of a differentiable simulator?
    \item \RQ{2} How does our algorithm compare to PPO/MAPPO and SHAC in both single-agent and multi-agent settings?
    \item \RQ{3} How does the performance of these algorithms change as the search space increases?
\end{compactitem}

\noindent Our contributions include:
\begin{compactitem}
    \item a novel RL framework (\fname{}) that employs learned gradient approximations, eliminating the need for differentiable simulators;
    \item the first empirical evaluation of SHAC in multi-agent environments, establishing a baseline for differentiable MARL;
    \item a comprehensive comparison across environments with varying agent counts and differentiability properties;
    \item experimental evidence demonstrating that \fname{} matches SHAC's performance in differentiable settings while substantially outperforming PPO in sample efficiency and final reward across both single-agent and complex multi-agent scenarios.
\end{compactitem}

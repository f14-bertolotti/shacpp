
\section{Conclusions \& Future Work}\label{sect:conclusion}

In this paper, we introduced \fname{}, an extension of SHAC, designed to provide a reinforcement learning solution that is sample-efficient, applicable in both single-agent and multi-agent settings, and capable of handling non-differentiable environments. We demonstrated that \fname{} preserves the core alignment with SHAC (\RQ{1}), consistently outperforms the state-of-the-art PPO algorithm in both single-agent and multi-agent tasks (\RQ{2}), and exhibits promising scalability in multi-agent environments, including the emergence of collaborative behaviors (\RQ{3}).

Despite these encouraging results, several avenues remain for future exploration: scaling beyond tens of agents, handling extreme partial observability (e.g., discovery scenarios with limited sensory input), and expanding evaluation to broader environments---particularly from the cooperative AI community~\cite{DBLP:journals/corr/abs-2211-13746} and adversarial settingsâ€”to further validate \fname{}'s robustness and versatility.

Addressing the cold-start problem more effectively could also enhance performance, particularly in environments with sparse or delayed rewards. Several promising techniques for mitigating this issue include:
\begin{enumerate*}[label=\emph{\roman*})]
    \item leveraging datasets generated by alternative algorithms or human demonstrations to initialize the world model before policy training begins (world-model pre-training).
    \item using Model Predictive Path Integral (MPPI)~\cite{Alvarez24}  to generate high-quality initial trajectories that can seed the world model and guide early exploration.
\end{enumerate*}
%
The latter approach may also be integrated into \fname{} during training to periodically introduce diverse trajectories, potentially improving robustness and convergence speed.

We also note that linear attention mechanisms, such as Performer~\cite{Choromanski20} and Mamba~\cite{Gu23}, could be leveraged to improve the scalability of \fname{} in environments with a large number of agents, without incurring the high computational cost associated with training classical Transformer architectures~\cite{Vaswani17}.


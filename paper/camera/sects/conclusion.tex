
\section{Conclusions \& Future Work}\label{sect:conclusion}

In this paper, we introduced \fname{}, an extension of SHAC, designed to provide a reinforcement learning solution that is sample-efficient, applicable in both single-agent and multi-agent settings, and capable of handling non-differentiable environments. We demonstrated that \fname{} preserves the core alignment with SHAC (\RQ{1}), consistently outperforms the state-of-the-art PPO algorithm in both single-agent and multi-agent tasks (\RQ{2}), and exhibits promising scalability in multi-agent environments, including the emergence of collaborative behaviors (\RQ{3}).

Despite these encouraging results, several avenues remain for future exploration. One key direction is scaling beyond the current limit of tens of agents. Another challenge lies in environments with extreme partial observability, such as the discovery scenario, where limited sensory input hinders learning. Expanding the evaluation to a broader set of environments—particularly those from the cooperative AI community~\cite{DBLP:journals/corr/abs-2211-13746} and adversarial settings—would further validate the robustness and versatility of \fname{}.

Addressing the cold-start problem more effectively could also enhance performance, particularly in environments with sparse or delayed rewards. Several promising techniques for mitigating this issue include:

\begin{itemize}
    \item \textbf{World model pre-training:} Leveraging datasets generated by alternative algorithms or human demonstrations to initialize the world model before policy training begins.
    \item \textbf{Model Predictive Path Integral (MPPI) control~\cite{Alvarez24}:} Using MPPI to generate high-quality initial trajectories that can seed the world model and guide early exploration.
\end{itemize}

The latter approach may also be integrated into \fname{} during training to periodically introduce diverse trajectories, potentially improving robustness and convergence speed.

We also note that linear attention mechanisms, such as Performer~\cite{Choromanski20} and Mamba~\cite{Gu23}, could be leveraged to improve the scalability of \fname{} in environments with a large number of agents, without incurring the high computational cost associated with training classical Transformer architectures~\cite{Vaswani17}.


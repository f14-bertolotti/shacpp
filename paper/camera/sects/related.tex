\section{Related Work}

\paragraph{Differentiable Engines}
Game engines~\cite{Bellemare13,Balla23} and Physics engines~\cite{Todorov12,Coumans16} have been widely used in the field of policy learning. Recently, there has been a trend towards differentiable engines, which allows for the use of gradient-based optimization methods, such as backpropagation. Notable examples are~\cite{Freeman21,Howell22,Genesis24}. These frameworks may be built on top of auto differentiation libraries such~\cite{Bradbury18,Ansel24} (this is the case of VMAS) or directly with analytical gradients~\cite{Carpentier18,Werling21}. Further, several works tackle the challenge of the non-smoothness of the contact dynamics~\cite{Degrave19,Moritz20}

\paragraph{Deep Reinforcement Learning}
Reinforcement learning has achieved significant advancements through the integration of deep neural networks.
Foundational works such as DQN~\cite{Mnih13} and its variants~\cite{Hessel18}
and AlphaZero~\cite{Silver17} achieved human-level performance in complex environments.
Subsequent algorithms improved efficiency, including A3C~\cite{Mnih16}, 
DDPG and PPO for continuous control~\cite{DBLP:journals/corr/LillicrapHPHETS15,Schulman17}, 
and SAC for off-policy learning~\cite{DBLP:conf/icml/HaarnojaZAL18}.
Model-based approaches like Dreamer~\cite{DBLP:conf/iclr/HafnerLB020} and PlaNet~\cite{DBLP:conf/icml/HafnerLFVHLD19} 
later enhanced sample efficiency.

\paragraph{Multi-Agent Reinforcement Learning}
Multi-Agent Reinforcement Learning (MARL) is a subfield of reinforcement learning that focuses on learning policies for multiple agents that interact with each other~\cite{albrecht2024multi}.
This field has a wide range of applications,
including robotics~\cite{DBLP:journals/air/ChungFYN24},
traffic control~\cite{DBLP:journals/tits/ChuWCL20},
and game playing~\cite{DBLP:conf/iclr/BakerKMWPMM20},
and involves different types of agents such as cooperative, competitive, and mixed.
In this paper, we primarily focus on cooperative multi-agent scenarios where agents collaborate to achieve common goals.
Prominent works include extensions of single-agent algorithms like MADDPG~\cite{DBLP:conf/nips/LoweWTHAM17} and MAPPO~\cite{DBLP:conf/nips/YuVVGWBW22},
as well as novel solutions such as COMA~\cite{DBLP:conf/aaai/FoersterFANW18} for credit assignment,
QMIX~\cite{DBLP:conf/icml/RashidSWFFW18} for coordination,
and Mean Field MARL~\cite{Yang18} for large-scale scenarios.

\paragraph{Reinforcement Learning on Differentiable Engines}
Precursors of SHAC with similar intuition are PODS~\cite{Mora21} and CE-APG~\cite{Gillen22}. In~\cite{Lv23}, the authors introduce rendering to enhance model-based RL\@.\ In~\cite{Zheng24} authors employ a differentiable symbolic expression search engine. Trajectory optimization approach is taken in~\cite{Wan24}. In~\cite{Georgiev24} authors tackle unstable gradient due to stiff dynamics by adaptively truncating trajectories when these occur. With respect to this approach, we take a substantially different method, by emulating the differentiable environment with a trained neural network.

\section{Background}

To provide a context for our work, we first introduce the reinforcement learning (RL) framework and the SHAC algorithm. We model each task as a finite-horizon Markov Decision Process (MDP) $\mathcal{M} = (\mathcal{S}, \mathcal{A}, \hat{F}, \hat{R}, \gamma, T)$, where $\mathcal{S}$ is the state space, $\mathcal{A}$ is the action space, $F:\mathcal{S}\times\mathcal{A}\rightarrow\mathcal{S}$ is the transition function, $R:\mathcal{S}\times\mathcal{A}\times\mathcal{S}\rightarrow\mathbb{R}$ is the reward function, $\gamma\in\mathcal{R}$ is the discount factor, and $T\in\mathbb{N}$ is the time horizon. 

The goal is to find a policy $\pi:\mathcal{S}\rightarrow\mathcal{A}$ that maximizes the expected return $J(\pi) = \mathbb{E}_{\tau\sim\pi}\left[\sum_{t=0}^{T-1}\gamma^t r(s_t, a_t, s_{t+1})\right]$, where $\tau = (s_0, a_0, s_1, a_1, \ldots)$ is a trajectory generated by the policy $\pi$.

In particular a SHAC policy is trained according the loss $\mathcal{L}_\theta^{\pi}$: 
$$ \frac{1}{NH}\sum_{i=1}^N \sum_{t=t_0}^{t_0+H-1} \gamma^{t-t_0} R(s_t^i, a_t^i) + \gamma^H V(s^i_{t_0+T})$$
Where $N$ is the number of sampled trajectories, $H << T$ is a time horizon, $s_t^i$ is the state at time $t$ of trajectory $i$, $a_t^i$ is the action at time $t$ of trajectory $i$, and $V(s):\mathcal{S}\rightarrow\mathbb{R}$ is the value function that estimates the expected return starting from state $s$. Further, SHAC assumes reward and transition functions to be differentiable, $\hat{F},\hat{R} \in \mathcal{C}^1$.

The value function is trained to approximate a temporal difference-$\lambda$ formulation \cite{Sutton98}, $\hat{V}$. In practice, the value function is trained to minimize the loss $\mathcal{L}_\theta^{V}$.

$$ \frac{1}{N}\sum_i^N\left|\left| V(s^i) - \hat{V}(s^i) \right|\right|^2 $$
$$ \hat{V}(s_t) = (1-\lambda) \left(\sum_{k=1}^{h-t-1}\lambda^{k-1}G_t^k\right) + \lambda^{h-t-1}G_t^{h-t}$$
$$ G_t^k = \left(\sum_{l=0}^{k-1}\gamma^l r_{t+l}\right) + \gamma^k V(s_{t+k})$$

$G^t_k$ represents the discounted sum of rewards up to step $k$, plus the bootstrapped value of the state at step $t+k$. The estimates $\hat{V}$ is computed by discounting longer (higher $k$) discounted sums, $G^k_t$.

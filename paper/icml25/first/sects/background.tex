
\section{Background \& Notation}\label{sect:background}

\subsection{Markov Decision Processes}
To provide context for our work, we first introduce the reinforcement learning (RL) framework. Each task is modeled as a partially observable, decentralized, finite-horizon, Markov Decision Process (MDP)~\cite{Shapley53}, $\mdp$: $(\agents,\states,\allo,\alla,\allp, \allr, \transition, \horizon, \discount, \initdist)$. Where $\agents=[\na]$ and $\states$ denote the set of agents and the state space, respectively. $\allo=\{\observations{i}\}_{i\in\agents}$ and $\alla=\{\actions{i}\}_{i\in\agents}$ are the observation space and action space for each agent. $\{\stateobs{i}:\states\rightarrow\observations{i}\}_{i\in\agents}$ are the projections from the state to the observation space and $\{\rewards{i}:\states\times\actions{i}\times\states\rightarrow\mathbb{R}\}_{i\in\agents}$ are the reward functions for each agent. $\transition:\states\times\alla$ denotes the transition function. $\discount$ and $\horizon$ denote the discount factor and the time horizon respectively. Finally, $\initdist$ is the initial state distribution.

Further, let us denote denote the space of trajectories, $\trajectories = \left(\states\times\alla\times\states\times\mathbb{R}^{n}\right)^*$, where each trajectory consists of a sequence of: starting state, action per agent, subsequent state, and reward per agent. Given a generic trajectory $\tau\in\trajectories$, we denote its $i$-th starting state with $s_i^\tau$, its $i$-th action for agent $j$ with $a_{ij}^\tau$, and its $i$-th reward for agent $j$ with $r_{ij}^\tau$. 

A policy for agent $i$ is a function $\policy{i}:\observations{i}\rightarrow\actions{i}$ that maps its observation space to its action space. We will denote the collective policy, determined by the collection of agent-specific policies simply with $\pi$. We can use the policy to sample trajectories from the MDP by iteratively applying the transition, reward, projection and policy functions. In this case, for trajectory $\tau$, we have that the $i$-th action for the $j$-th agent is $a_{ij}^\tau = \policy{j}(\stateobs{j}(s_i^\tau))$ and the next state is $s_{i+1}^\tau = \transition(s_i^\tau,a_{i1},\dots,a_{i\na})$. The $i$-th reward for the $j$-th agent is $r_{ij}^\tau = \rewards{j}(s_i^\tau,a_{ij}^\tau,s_{i+1}^\tau)$. Finally, $s_0$ is sampled from the initial state distribution $\initdist$. For brevity, we will simply denote with $\tau\sim\pi$ a trajectory sampled following the policies.

The goal of the MDP is to find a policy that maximizes the expected return 
$$\mathbb{E}_{\tau\sim\pi}\left[\sum_{t=0}^{\horizon-1}\sum_{i\in\agents}\gamma^t r_{ti}^\tau \right]$$

\subsection{Neural Networks}
A neural network is a function $f_{\theta}:\mathbb{R}^n\rightarrow\mathbb{R}^m$ parameterized by $\theta$ that maps an input vector $x\in\mathbb{R}^n$ to an output vector $y\in\mathbb{R}^m$. The parameters $\theta$ of the network are learned by minimizing a loss function $\mathcal{L}_\theta$ with respect to the training data.

In this work, we will often approximate functions (such as transition $\transition$) with neural networks. Therefore, for each function $g$, we will denote the corresponding neural network with $g_\theta$. For instance, the transition and its corresponding neural network are denoted as $F$ and $F_{\theta}$, respectively. The corresponding loss will be denoted as $\mathcal{L}_\theta^g$ and it will aim to minimize the differences between $g$ and $g_\theta$.

While we use a unique set of parameters $\theta$ for all the networks. Each network can either use its own subset of $\theta$ or it can share parameters with other networks. In practice, the parameters will be shared only for the policies.

For simplicity, we formally treat only the case where the policies observe the full state, meaning that $\stateobs{i}$ is the identity function. However, in our experiments, we consider also scenarios where the full state is not available to the policies.

\subsection{SHAC}

In its trivial multi-agent extension, SHAC trains policies ${\pi_1}_\theta,\dots,{\pi_\na}_\theta$ by minimizing the loss function $\mathcal{L}_\theta^{\pi}:\trajectories^\bs\rightarrow\mathbb{R}$ for a batch of $\bs$ trajectories $\batch = \{\tau_1, \ldots, \tau_\bs\}$ sampled from $\mdp$. The loss $\mathcal{L}_\theta^{\pi}(\batch)$ is defined as:  
\begin{equation*}\label{eq:shac}
    \frac{1}{\bs\na\mh}\sum_{\tau\in\batch} \sum_{t=t_0}^{t_0+\mh-1} \sum_{i\in\agents} \discount^{t-t_0} R(s_{t}^\tau, a_{ti}^\tau) + \discount^\mh \valuef_\theta(s^\tau_{t_0+\mh})
\end{equation*}
where $\mh \ll \horizon$ is the roullout size, $t_0$ is the initial time step of the rollout, $\valuef_\theta: \states \rightarrow \mathbb{R}$ is the value function estimating the expected return. It is apparent that SHAC assumes that the policy, reward, value, projection, and transition functions are all differentiable, i.e., $\{\policy{i}\}_{i\in\agents},\{\rewards{i}\}_{i\in\agents},\valuef_\theta,\transition \in \mathcal{C}^1$. While this is a common requirement for policy and value functions, differentiable transition and reward functions are rarely available. 

The value function $\valuef_\theta$ is trained to approximate a temporal difference-$\lambda$ formulation \cite{Sutton98} $\valuef$, which in SHAC translates to minimize the loss $\mathcal{L}_\theta^{\valuef}:\trajectories^\bs\rightarrow\mathbb{R}$.

$$ \mathcal{L}_\theta^\valuef(\batch)=\frac{1}{\bs\mh}\sum_{\tau\in\batch}\sum_{t=0}^\mh\left|\left| \valuef_\theta(s^\tau_t) - \valuef(s^\tau_t) \right|\right|^2 $$
$$ \valuef(s_t) = (1-\lambda) \left(\sum_{k=1}^{h-t-1}\lambda^{k-1}G_t^k\right) + \lambda^{h-t-1}G_t^{h-t}$$
$$ G_t^k = \left(\sum_{l=t}^{k-1+t}\sum_{i\in\agents}\gamma^{l-t} r_{li}\right) + \gamma^k V(s_{t+k})$$

$G^t_k$ represents the discounted sum of rewards up to step $k$, plus the bootstrapped value of the state at step $t+k$. The estimates $V$ is computed by discounting longer (higher $k$) discounted sums, $G^k_t$.

Finally, \Cref{fig:shac} presents an unrolled view of the SHAC algorithm for a single-agent scenario.

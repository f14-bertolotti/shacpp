\section{Background}

\subsection{Markov Decision Processes}
To provide context for our work, we first introduce the reinforcement learning (RL) framework. Each task is modeled as a finite-horizon Markov Decision Process (MDP) $\mathcal{M} = (\mathcal{S}, \mathcal{A}, \hat{F}, \hat{R}, \gamma, T)$, where $\mathcal{S}$ represents the state space, $\mathcal{A}$ denotes the action space, $\hat{F}:\mathcal{S} \times \mathcal{A} \rightarrow \mathcal{S}$ is the transition function, $\hat{R}:\mathcal{S} \times \mathcal{A} \times \mathcal{S} \rightarrow \mathbb{R}$ is the reward function, $\gamma \in [0, 1]$ is the discount factor, and $T \in \mathbb{N}$ is the time horizon.

Further, let $\mathcal{T}$ denote the space of trajectories, defined as $\mathcal{T} = (\mathcal{S} \times \mathcal{A} \times \mathcal{S} \times \mathbb{R})^*$, where each trajectory consists of a sequence of starting states, actions, subsequent states, and rewards. We will use the symbols $((s_0, a_0, s_1, r_0), (s_1, a_1, s_2, r_1), \ldots)$ to denote a trajectory in $\mathcal{T}$.

The goal of the MDP is to find a policy $\pi:\mathcal{S}\rightarrow\mathcal{A}$ that maximizes the expected return 
$$J(\pi) = \mathbb{E}_{\tau\sim\pi}\left[\sum_{t=0}^{T-1}\gamma^t r(s_t, a_t, s_{t+1})\right]$$
Where $\tau \in \mathcal{T}$ is a trajectory generated by the policy $\pi$ with the transition function $\hat{F}$.


\subsection{SHAC}

To solve an MDP $\mathcal{M} = (\mathcal{S}, \mathcal{A}, \hat{F}, \hat{R}, \gamma, T)$, SHAC trains a policy $\pi$ by minimizing the loss function $\mathcal{L}_\theta^{\pi}:\mathcal{T}^N\rightarrow\mathbb{R}$:  
$$\mathcal{L}_\theta^{\pi} = \frac{1}{NH}\sum_{i=1}^N \sum_{t=t_0}^{t_0+H-1} \gamma^{t-t_0} R(s_t^i, a_t^i) + \gamma^H V(s^i_{t_0+T}),$$  
where $N$ is the number of sampled trajectories, $H \ll T$ is a time horizon, $s_t^i$ is the state at time $t$ for trajectory $i$, $a_t^i$ is the action taken at time $t$ for trajectory $i$, and $V(s): \mathcal{S} \rightarrow \mathbb{R}$ is the value function estimating the expected return starting from state $s$. Additionally, SHAC assumes that the policy, reward, and transition functions are differentiable, i.e., $\pi, \hat{F}, \hat{R} \in \mathcal{C}^1$. 

The value function is trained to approximate a temporal difference-$\lambda$ formulation \cite{Sutton98} $\hat{V}$, which in SHAC translates to minimize the loss $\mathcal{L}_\theta^{V}:\mathcal{T}^N\rightarrow\mathbb{R}$.

$$ \frac{1}{N}\sum_i^N\left|\left| V(s^i) - \hat{V}(s^i) \right|\right|^2 $$
$$ \hat{V}(s_t) = (1-\lambda) \left(\sum_{k=1}^{h-t-1}\lambda^{k-1}G_t^k\right) + \lambda^{h-t-1}G_t^{h-t}$$
$$ G_t^k = \left(\sum_{l=0}^{k-1}\gamma^l r_{t+l}\right) + \gamma^k V(s_{t+k})$$

$G^t_k$ represents the discounted sum of rewards up to step $k$, plus the bootstrapped value of the state at step $t+k$. The estimates $\hat{V}$ is computed by discounting longer (higher $k$) discounted sums, $G^k_t$.

Finally, Figure~\ref{fig:shac} presents an unrolled view of the SHAC algorithm.

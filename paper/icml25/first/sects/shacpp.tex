\section{\fname{}}

As mentioned previously, we aim to address the limitations of SHAC by approximating the gradients of a differentiable simulator with neural networks. In particular, we aim to train a neural network to approximate the reward function by minimizing: 
$$ \mathbb{E}_{s,a}\left[\left|\left| R_{\theta}(s, a) - \hat{R}(s, a) \right|\right|^2\right] $$

In practice, this is achieved by caching the state-action-reward triplets obtained from the environment and then training the neural network to minimize the MSE loss. That is, we minimize the loss $\mathcal{L}_\theta^{R}$:
$$ \frac{1}{\left|B\right|}\sum_{s,a \in B}\left|\left| R_\theta(s, a) - \hat{R}(s, a) \right|\right|^2 $$
Where $B$ represents a batch of state-action-reward triplets sampled uniformly from the cache.

Similarly, we aim to train a neural network to approximate the transition function by minimizing. However, to avoid vanishing or exploding gradients, we employ a non-recursive approach similar similar to Action World Model\cite{Ma24}. This corresponds to train a neural network that given a starting observation from $\mathcal{S}$ and a sequence of $N$ actions from $\mathcal{A}^N$, predicts the next $N$ observations from $\mathcal{S}^N$. In practice, we aim to minimize:

$$ \mathbb{E}_{\tau}\left[\sum_{i=1}^N \left|\left| F_{\theta}(s^0, a^0, \dots, a^N)^i - \hat{F}(s^i, a^i) \right|\right|^2\right] $$

Where $\tau$ corresponds to a trajectory $(s^0, a^0, s^1, a^1, \dots, s^N, a^N)$. In practice, this is achieved by caching trajectories and then training the neural network to minimize the MSE loss. That is, we minimize the loss $\mathcal{L}_\theta^{F}$:
$$ \frac{1}{\left|B\right|}\sum_{\tau \in B}\sum_{i=1}^N\left|\left| F_{\theta}(s^0, a^0, \dots, a^N)^i - \hat{F}(s^i, a^i) \right|\right|^2 $$
Where $B$ represents a batch of trajectories sampled uniformly from the cache.

While SHAC is aims to minimize $\mathcal{L}\theta^\pi$ and $\mathcal{L}_\theta^V$, we also aim to minimize also $\mathcal{L}_\theta^R$ and $\mathcal{L}_\theta^F$. Further, we modify the SHAC policy loss, $\mathcal{L}_\theta^{\pi}$, to include a term that penalizes actions outside the action space. That is, suppose the action space is $(a,b)$ (with $a<b$), we minimize the loss $\mathcal{L'}_\theta^\pi:$

$$\mathcal{L}_\theta^\pi + \alpha \frac{1}{NH}\sum_{i=0}^N\sum_{t=t_0}^{t_0+H-1} d(a_t^i,(a,b))$$
$$d(x,(a,b)) = \begin{cases}(a-x)^2 & \text{if } x < a \\ (x-b)^2 & \text{if } x > b \\ 0 & \text{otherwise} \end{cases}$$

Where $\alpha$ is a hyper-parameter that controls the strength of the penalty. In our experiments, we set $\alpha=1$. In practice, actions are usually either clipped to the allowed action space or, an activation function is such as hyperbolic tangent is used to ensure that the actions are within the action space. This is acceptable for PPO and SHAC. However, in our scenario, this penalty is crucial. To understand why, consider that we are training the policy to optimize two neural networks that approximate the environment. Further, these networks are not trained on data generated from action inside the action space. As a result, it is very likely that these networks achieve their maximum for actions outside the allowed range. Leading to higher than normal pre-activation actions that disrupt the training process.

To summarize, our algorithm, aims to minimize the 4 presented losses, $\mathcal{L'}_\theta^{\pi}$, $\mathcal{L}_\theta^V$, $\mathcal{L}_\theta^R$, and $\mathcal{L}_\theta^F$. A minimal pseudocode of our algorithm is presented in \ref{alg:shacpp}.

However, consider that, to ensure fast convergence of the reward and transition networks, we employ a cache to store seen trajectories. To ensure that the cache is balanced, and not filled completely with $0$-reward trajectories, we divide the cache $K$ equally sized bins. Each bin is filled with trajectories that have a similar reward. For the reward network, this corresponds to the immediate reward. For the transition network, this corresponds to the sum of reward accumulated during the trajectory.

And, to minimize the time spent between swapping among the different training procedures (policy, value, reward, transition), we apply a cool-down of $K^V$/$K^R$/$K^F$ episodes between running training procedure for the value/reward/transition networks. 

The full implementation is openly available at 
\begin{center}
\url{https://redacted/for/anonymity}
\end{center}

\begin{algorithm}[t]
    \begin{algorithmic}[1]
    \STATE Initialize $\pi_\theta,V_\theta,R_\theta,W_\theta$ networks
    \STATE Initialize learning rates $\eta_\pi, \eta_R, \eta_V, \eta_W$
    \STATE Initialize environment $\mathcal{E}$
    \STATE Initialize $\gamma, \alpha$
    \FOR{episode in $1, \ldots, N_{\text{episodes}}$}
    
        
        \STATE $s,a,r,v \sim \mathcal{E}(\pi)$
        \STATE $\hat{s} = W(s[0], a)$
        \STATE $\hat{r} = R(\hat{s})$
        \STATE $\hat{v} = V(\hat{s})$
        
        \STATE $\pi_\theta \gets \pi_\theta - \eta_\pi \nabla \mathcal{L'}^\pi_\theta(\hat{r},\hat{v},\gamma,\alpha)$
        \STATE $V_\theta \gets V_\theta - \eta_V \nabla \mathcal{L}^V_\theta(\hat{v},s,v)$
        \STATE $R_\theta \gets R_\theta - \eta_R \nabla \mathcal{L}^R_\theta(\hat{r},s,r)$
        \STATE $W_\theta \gets W_\theta - \eta_W \nabla \mathcal{L}^F_\theta(\hat{s},s,a)$
    
    \ENDFOR
    
    \end{algorithmic}
    \label{alg:shacpp}
    \caption{SHAC++ minimal (no cache and no cool-down) pseudocode}
\end{algorithm}



\section{Introduction}

Learnign control policies has important applications in robotics~\cite{Singh22}, autonomous driving~\cite{Elallid22}, swarm intelligence~\cite{Tang21} and many other fields. 
These scenarios are varied, with can consider both single and multi-agent systems, and can have different dynamics and reward functions.
Current approaches often focus on either single- or multi-agent scenarios, lacking a unified solution that works effectively in both settings.
Moreover, reinforcement learning algorithms tend to be sample inefficient due to the high variance of the gradient estimates and the exploration required.
A recently introduced method for the latter problem is SHAC~\cite{Xu22} which exploits differentiable simulators to provide precise gradient information, thereby expediting policy optimization and improving learning efficiency.

Despite its effectiveness, the main limitations of SHAC are twofold. Firstly, it requires a differentiable simulator and differentiable reward function to work properly. Both of which may be not feasible to obtain in practice, especially with inherently sparse rewards. Secondly, even when a differentiable simulator is available, SHAC has been shown to be sensitive to complex dynamics such as object collisions~\cite{Georgiev24} or multi-agent interactions, which is currently an unsplore area in the literature.

In this work, we aim to address both these limitations by approximating the gradients of the differentiable simulator. In particular, we will replace both the simulator and the reward function with a neural network trained in tandem with the policy network and value network. Overall, this will provide an algorithm that is more robust to complex dynamics, sparse rewards, and can work in a wider range of environments, from sigle-agent to multi-agent scenarios.
On the other hand, however, this will also introduce additional complexity and computational cost tied to training additional networks.

We evaluate our framework---dubbed \fname{}---against PPO~\cite{Schulman17}, a state-of-the-art algorithm for non-differentiable environments (used also in multi-agent scenarios with the MAPPO variant~\cite{DBLP:conf/nips/YuVVGWBW22}), and SHAC~\cite{Xu22}, designed for differentiable environments. 
We compare these algorithms on VMAS---a state-of-the-art simulator for complex physics-based multi-agent scenarios~\cite{DBLP:conf/dars/BettiniKBP22}---across cooperative tasks with varying numbers of agents, enabling a systematic assessment of how their performance scales as the search space grows.

With this work, we aim to answer the following research questions:
\begin{compactitem}
    \item \RQ{1} Can we train a neural network to approximate the gradients of a differentiable simulator?
    \item \RQ{2} How does our algorithm compare to PPO and SHAC in both single-agent and multi-agent settings?
    \item \RQ{3} How does the performance of these algorithms change as the search space increases?
\end{compactitem}

We summarize our contributions as follows:
\begin{compactitem}
    \item We propose a new reinforcement learning framework that lift the requirement for a differentiable simulator in SHAC.
    \item We assess the SHAC algorithm in multi-agent scenarios, where it has not been previously evaluated.
    \item We evaluate our algorithm against PPO and SHAC on a set of environments with increasing numbers of agents.
    \item We show that our algorithm achieve SHAC-like performance in differentiable environments and outperform PPO in non-differentiable environments.
\end{compactitem}

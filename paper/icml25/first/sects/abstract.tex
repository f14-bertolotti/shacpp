\begin{abstract}
In this work, we introduce a novel framework, \fname{}, inspired by SHAC~\cite{Xu22}, a reinforcement learning algorithm designed for differentiable environments. Unlike SHAC, \fname{} eliminates the requirement for differentiable transition and reward functions by utilizing plain neural networks to estimate the gradients that would otherwise be derived directly from the environment. These neural networks are trained alongside the policy and value networks of SHAC. We evaluate \fname{} on a set of challenging multi-agent scenarios from VMAS~\cite{DBLP:conf/dars/BettiniKBP22} and compare its performance against SHAC and PPO. Our results demonstrate that \fname{} significantly outperforms PPO and achieves comparable performance to SHAC, despite not having access to environment gradients.
\end{abstract}

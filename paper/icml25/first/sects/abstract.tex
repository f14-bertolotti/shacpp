\begin{abstract}
Reinforcement learning algorithms have demonstrated success in domains such as robotics and multi-agent systems, 
yet they often exhibit limited sample efficiency in complex environments, 
and a unified approach that effectively addresses both single- and multi-agent settings remains elusive.
To challenge this gap, we introduce a novel framework, \fname{}, inspired by SHAC, a reinforcement learning algorithm designed for differentiable environments. Unlike SHAC, \fname{} eliminates the requirement for differentiable transition and reward functions by utilizing plain neural networks to estimate the gradients that would otherwise be derived directly from the environment. These neural networks are trained alongside the policy and value networks of SHAC. We evaluate \fname{} on a set of challenging multi-agent scenarios from VMAS and compare its performance against SHAC and PPO---a current state-of-the-art algorithm for non-differentiable environments. Our results demonstrate that \fname{} significantly outperforms PPO and achieves comparable performance to SHAC, despite not having access to environment gradients.
\end{abstract}

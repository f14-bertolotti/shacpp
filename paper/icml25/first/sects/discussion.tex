\section{Discussion}
In the following, we discuss the results obtained in \Cref{sect:experiments} and provide some insights into the performance of \fname{} compared to SHAC and PPO trying to answer the research questions posed in \Cref{sect:introduction}. Note that \fname{} does not aim to outperform SHAC, it merely aims to perform comparably with SHAC while being applicable even in non-differentiable environment. 
\begin{center}
\RQ{1} \textbf{Can we train a neural network to approximate the gradients of a differentiable simulator?}
\end{center}

The proposed framework, \fname{}, appears to be successful in emulating the behavior of SHAC in differentiable environments such as transport and sampling. Therefore, we can conclude that it is definitely possible to approximate the gradients of a differentiable environment with a neural network. However, we note that this approach, while obtaining comparable results in most setups, it tends to display higher variance. Both neural networks representing the reward and transition functions require sufficient training before becoming effective, which may entail multiple episodes. Interestingly, \fname{} outperforms SHAC in the Sampling scenario. A possible explanation is that the complex dynamics of the environment may lead to unstable gradients.

\begin{center}
\RQ{2} \textbf{How does our algorithm compare to PPO and SHAC in both single-agent and multi-agent settings?}
\end{center}
The proposed algorithm appears capable of outperforming PPO across all scenarios showing also a better sample efficiency. In single-agent cases, while PPO manages to find near-good policies (e.g., in the Dispersion scenario), it often fails to complete scenarios within the same number of steps. Indeed, \fname{} achieves better results in all environments (see \Cref{tab:max-rewards}). Despite having to train multiple networks, \fname{} converges much faster to optimal policies, as gradient guidance enables quicker and more accurate convergence to the desired outcome. This pattern becomes even more evident in multi-agent scenarios. In these cases, PPO often struggles to find even marginally good policies. \fname{}, on the other hand, eventually succeeds in understanding the environment and subsequently improves the policy to enable cooperation---see both \Cref{tab:max-rewards} and \Cref{fig:experiments}.

\begin{center}
\RQ{3} \textbf{How does the performance of these algorithms change as the search space increases?}
\end{center}
To analyze the impact of increasing search spaces, we primarily focused on scenarios with varying numbers of agents. The results demonstrate that performance patterns differ across scenarios. In the Dispersion scenario, \fname{} shows increasing difficulty in finding optimal policies as the number of agents grows. Conversely, in Transport and Sampling scenarios, the system appears to converge more quickly with more agents. This counterintuitive behavior likely stems from the fact that, despite larger search spaces, the fundamental tasks in Sampling and Transport become relatively easier to solve with multiple agents, making policy discovery more straightforward. Even with approximated world dynamics (transition and reward functions), the guidance provided is sufficient to lead policies toward optimal behavior.

This advantage does not hold in the Dispersion scenario, where complexity increases substantially with additional agents. Nevertheless, we observe the emergence of cooperation across all scenarios, suggesting potential scalability to larger agent populations. The main bottleneck lies in the transformer architecture for the action world model, where the sequence length scales with the number of agents (sequence length = num. agents times num. train steps). With significantly large populations (~50 agents), the attention mechanism becomes computationally expensive, leading to prohibitively long memory requirements. In these scenarios, a linear attention mechanism (such as \cite{Beltagy20}) may be employed.

The Discovery scenario, exhibiting high variance, shows no clear pattern compared to the other three scenarios.
This might be attributed to partial observability, which significantly complicates convergence and inter-agent cooperation.
